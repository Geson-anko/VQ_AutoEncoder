{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to VQ AutoEncoder clutstering tutorial!\n",
    "ここでは、VQ (Vector Quantized) AutoEncoderによるでは、教師なしクラスタリング手法を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意事項\n",
    "このチュートリアルは演算負荷が高いためGPU環境が必要です。  \n",
    "Google Colab で実行している方は、ページ上部から**ランタイム** &rarr; **ランタイムのタイプを変更** をクリックし **ハードウェアアクセラレータ** を *None* から *GPU* に変更してください。  \n",
    "GPUが使用可能かどうかは次のコードブロックを実行することで分かります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"GPU:\",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインストールとインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning \n",
    "!pip install torchsummaryX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import pytorch_lightning as pl\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from torchvision.utils import make_grid\n",
    "from typing import *\n",
    "from datetime import datetime\n",
    "from torchsummaryX import summary\n",
    "from torch.utils import data as dutil\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アルゴリズムの説明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VQ (Vector Quantization 日本語ではベクトル量子化) とは何か\n",
    "この[サイト](https://aidiary.hatenablog.com/entry/20120813/1344853878)\n",
    "より\"VQ\"とは、ベクトルで表されたデータ集合を有限個の代表的なパターン（セントロイド/量子化ベクトル）に置き換えることです。  \n",
    "と、定義を言われても何かよくわかりませんよね。図を使って説明しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはデータ点があります。なんとなくまとまりがありそうですね。 \n",
    "<br> \n",
    "<br> \n",
    "<img src=\"imgs/colab1.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そしてデータ点の中からでもランダムでもいいのですが、適当に量子化ベクトル（セントロイド）を置きます。この量子化ベクトルの数を**量子化数**と呼びます。  \n",
    "<br>\n",
    "<img src=\"imgs/colab2.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして各データ点を、最も近い量子化ベクトルで置き換えます。これで量子化は完了しました。  \n",
    "<br>\n",
    "<img src=\"imgs/colab3.jpeg\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そしてデータ点に「一番目の量子化ベクトル」のようにラベルを割り振れば、クラスタリングとなります。  \n",
    "データ点の分布に合わせてこの量子化ベクトルが適切な位置にあれば、うまくクラスタリングできていると言えますね！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VQ AutoEncoder clusteringとは\n",
    "Encoderの出力を量子化ベクトルを使ってクラスタリングすることで、画像や音声などのデータに対し**教師なしで**ラベルを割り振ります。そのAutoEncoderの学習と、量子化ベクトルの学習（最適な位置に移動すること）を同時行うことが特徴です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### なぜVQ AE clusteringをするのか\n",
    "ここまで読んできて、「k-means法で十分ではないか？」と疑問を持つ方もいるでしょう。この手法の目的は２つあります。  \n",
    "1.  勾配降下法のみを用いてクラスタリングを行う。  \n",
    "k-means法は「クラスターの平均値にセントロイドを移動させる」というアルゴリズムですから、深層学習のモデルに組み込む際に更新のアルゴリズムを分離する必要があります。VQAEでは全て勾配降下法により学習するためとても簡潔です。\n",
    "2.  様々な「距離」の概念を使えるようにする。  \n",
    "k-means法は基本的にユークリッド距離を用いることを前提にアルゴリズムが構築されています。しかし深層学習では多くの場面でコサイン類似度などの非ユークリッド距離を用います。VQ AEは「距離」という概念を直接使うため、k-means法では組み込みにくいデータパターンに対応できるのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル（VQ AEの作り方）\n",
    "VQ AutoEncoderを作るためには次のステップがあります。  \n",
    "1. AutoEncoder(Encoder-Decoder) を作成する\n",
    "2. 量子化ベクトルの数 x Encoderの出力次元数(Quantizing Dim) の重みを持つ`Quantizing Layer`を作る\n",
    "3. `Encoder`, `Decoder`, `Quantizing Layer`を組み合わせて`VQ AutoEncoder`をつくる\n",
    "4. 損失（誤差）関数を定義し、学習する  \n",
    "<br>\n",
    "さあ作っていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder\n",
    "今回はMNIST手書き数字データセットを用いるので、入力は`BATCH x 1 x 28 x 28`になります。別の画像データにもモデルを対応させられるようにするために、ハイパーパラメータ`h`にその値を格納しています。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Encoder`は1x28x28の画像を 784 に平坦化します。その後は順にデータの次元数を小さくしています。  \n",
    "> 1x28x28 &rarr; 784 &rarr; 256 &rarr; 128 &rarr; quantizing_dim\n",
    "\n",
    "最後のレイヤーを`Tanh`にすることで、出力の範囲を -1 ~ 1 に納めています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,h):\n",
    "        super().__init__()\n",
    "        self.channels = h.channels\n",
    "        self.width = h.width\n",
    "        self.height = h.height\n",
    "        self.input_size = (1,h.channels,h.width,h.height)\n",
    "        self.output_size = (1,h.quantizing_dim)\n",
    "        self.indim = h.channels * h.width* h.height\n",
    "        self.h = h\n",
    "        self.Eout_dim = h.quantizing_dim\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(self.indim,256),nn.ReLU(),\n",
    "            nn.Linear(256,128),nn.ReLU(),\n",
    "            nn.Linear(128,self.Eout_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        y = self.layers(x)\n",
    "        return y\n",
    "    \n",
    "    def summary(self):\n",
    "        dummy = torch.randn(self.input_size)\n",
    "        summary(self,dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecoderはEncoderと逆のことをします。Encoderの出力を展開し、元々の画像データに復元します。  \n",
    "> quantizing_dim &rarr; 128 &rarr; 256 &rarr; 784 &rarr; 1x28x28  \n",
    "\n",
    "最後のレイヤーを`Sigmoid`にすることで、出力の範囲を 0 ~ 1 にしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,h):\n",
    "        super().__init__()\n",
    "        self.channels = h.channels\n",
    "        self.width = h.width\n",
    "        self.height = h.height\n",
    "        self.input_size = (1,h.quantizing_dim)\n",
    "        self.output_size = (1,h.channels,h.width,h.height)\n",
    "        self.h = h\n",
    "        self.Dout_dim = self.channels*self.width*self.height\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(h.quantizing_dim,128),nn.ReLU(),\n",
    "            nn.Linear(128,256),nn.ReLU(),\n",
    "            nn.Linear(256,self.Dout_dim),nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x:torch.Tensor):\n",
    "        y = self.layers(x)\n",
    "        y = y.view(-1,self.channels,self.height,self.width)\n",
    "        return y\n",
    "\n",
    "    def summary(self):\n",
    "        dummy = torch.randn(self.input_size)\n",
    "        summary(self,dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantizing layer\n",
    "本日の主役、`Quantizing Layer (量子化層)`です！このレイヤーは`量子化ベクトルの個数 x Encoderの出力次元数`の学習可能な重みを持ちます。そして入力されたデータを最も近い量子化ベクトルに置換し、ついでに何番目の量子化ベクトルに置換されたかを表すidxも返します。このidxがクラスタリングされたラベルとなるのです！  \n",
    "<img src=\"imgs/Qflow.jpeg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　Quantizing Layerの重みの初期化時には、データと同じ分布に量子化ベクトルを用意する必要があります。なぜなら、量子化ベクトルを単純にランダムに取っても、データ点の分布と全く違う場所では量子化できないからです。  \n",
    "　下のコードブロックでは、`initialized_by_dataset`という引数をTrueにすることで、データ点から直接重みを初期化することができます。もし、Quantizing Layerに入力されるデータの分布（Encoderの出力分布）が**正規分布と仮定されるのであれば**、`initialized_by_dataset=False`にしても大丈夫でしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizing(nn.Module):\n",
    "\n",
    "    __initialized:bool = True\n",
    "\n",
    "    def __init__(\n",
    "        self, num_quantizing:int, quantizing_dim:int, _weight:torch.Tensor = None,\n",
    "        initialize_by_dataset:bool = True, mean:float = 0.0, std:float = 1.0,\n",
    "        dtype:torch.dtype = None, device:torch.device = None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert num_quantizing > 0\n",
    "        assert quantizing_dim > 0\n",
    "        self.num_quantizing = num_quantizing\n",
    "        self.quantizing_dim = quantizing_dim\n",
    "        self.initialize_by_dataset = initialize_by_dataset\n",
    "        self.mean,self.std = mean,std\n",
    "\n",
    "        if _weight is None:\n",
    "            self.weight = nn.Parameter(\n",
    "                torch.empty(num_quantizing, quantizing_dim ,dtype=dtype,device=device)\n",
    "            )\n",
    "            nn.init.normal_(self.weight, mean=mean, std=std)\n",
    "\n",
    "            if initialize_by_dataset:\n",
    "                self.__initialized = False\n",
    "                self.__initialized_length= 0\n",
    "\n",
    "        elif type(_weight) is torch.Tensor:\n",
    "            assert _weight.dim() == 2\n",
    "            assert _weight.size(0) == num_quantizing\n",
    "            assert _weight.size(1) == quantizing_dim\n",
    "            self.weight = nn.Parameter(_weight.to(device).to(dtype))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Weight type is unknown type! {}\".format(type(_weight)))\n",
    "\n",
    "    def forward(self,x:torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x   : shape is (*, E), and weight shape is (Q, E). \n",
    "        return -> ( quantized : shape is (*, E), quantized_idx : shape is (*,) )\n",
    "        \"\"\"\n",
    "        input_size = x.shape\n",
    "        h = x.view(-1,self.quantizing_dim) # shape is (B,E)\n",
    "        \n",
    "        self.initialize_weight(x)\n",
    "        \n",
    "        delta = self.weight.unsqueeze(0) - h.unsqueeze(1) # shape is (B, Q, E)\n",
    "        dist =torch.sum(delta*delta, dim=-1) # shape is (B, Q)\n",
    "        q_idx = torch.argmin(dist,dim=-1) # shape is (B,)\n",
    "        q_data = self.weight[q_idx] # shape is (B, E)\n",
    "\n",
    "        return q_data.view(input_size), q_idx.view(input_size[:1])\n",
    "    \n",
    "    def initialize_weight(self, x:torch.Tensor):\n",
    "        h = x.view(-1,self.quantizing_dim) # shape is (B,E)\n",
    "        if not self.__initialized and self.initialize_by_dataset:\n",
    "            getting_len = self.num_quantizing - self.__initialized_length\n",
    "            init_weight = h[torch.randperm(len(h))[:getting_len]]\n",
    "            \n",
    "            _until = self.__initialized_length + init_weight.size(0)\n",
    "            self.weight.data[self.__initialized_length:_until] = init_weight\n",
    "            self.__initialized_length = _until\n",
    "            print('replaced weight')\n",
    "\n",
    "            if _until >= self.num_quantizing:\n",
    "                self.__initialized = True\n",
    "                print('initialized')\n",
    "    \n",
    "    @property\n",
    "    def is_initialized(self):\n",
    "        return self.__initialized\n",
    "\n",
    "    @is_initialized.setter\n",
    "    def is_initialized(self, b:bool):\n",
    "        self.__initialized = b\n",
    "        if b:\n",
    "            self.__initialized_length= num_quantizings\n",
    "        else:\n",
    "            self.__initialized_length = 0\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQ AutoEncoder\n",
    "もはや材料は揃いました。VQ AutoEncoderを構築していきましょう。今回作るモデルの概要はこちらです。\n",
    "<img src=\"imgs/vqae.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoderの出力である$E_{out}$はQuantizing Layerに入力され、最も近い量子化ベクトルである$Q_{out}$になり出力されます。(ついでに量子化ベクトルのインデックスも!)  \n",
    "Decoderはいつも通り$E_{out}$を受け取り再構成して画像を出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 損失（誤差）関数の取り方\n",
    "今回の損失は`reconstruction_loss(再構成誤差)` と `quatizing_loss(量子化誤差)`の和を使用します。  \n",
    "`再構成誤差`はいつも通りAutoEncoderの入力画像と出力画像のMSE(平均二乗誤差）を使い、それを最小化します。 \n",
    "`量子化誤差`は、$E_{out}$と$Q_{out}$の*距離*を使い、それを最小化します。$E_{out}$は変化せず、$Q_{out}$の方が学習されます。（Encoderに対してはbackpropagation(誤差逆伝搬)を行わない)  \n",
    "<br>\n",
    "今回`量子化誤差`は`Quantizing Layer`でユークリッド距離を使用しているため、それに近い距離の概念であるMSE(平均二乗誤差)を使用します。\n",
    "$Q_{out}$が$E_{out}$と全く同じなりそうですが、実際一つの量子化ベクトルは複数のデータ点と*距離*を最小化しようとするため、だいたい *距離* が平均的に最小になる位置に量子化ベクトルは移動します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 記録について\n",
    "記録するものはたくさんあります。全てTensorboardに記録されます。\n",
    "- 学習時のハイパーパラメータ  \n",
    "  学習率(learning rate)などを記録します。  \n",
    "  <br>\n",
    "- 再構成誤差(reconstruction_loss/r_loss)  \n",
    "  これは純粋にAutoEncoderの損失です。  \n",
    "  <br>\n",
    "- 量子化誤差(quantizing_loss/q_loss)   \n",
    "  量子化ベクトルとEncoderの出力との平均距離です。  \n",
    "  <br>\n",
    "- 損失(loss)  \n",
    "  loss = r_loss + q_loss  \n",
    "  <br>\n",
    "- 量子化再構成誤差(reconstructed_quantizing_loss/rq_loss)  \n",
    "  これは量子化層の出力をDecoderに通し、再構成した画像とEncoderの入力画像との誤差です。  \n",
    "  <br>\n",
    "- 画像の記録 \n",
    "  下のような形で記録されます。上から、入力画像、再構成画像、量子化後($Q_{out}$)の再構成画像となっています。 \n",
    "  <br>\n",
    "  <img src=\"imgs/logimg.png\" height=\"100\">  \n",
    "  <br>\n",
    "- １つの量子化ベクトルごとの割り当て数\n",
    "  下のような形で記録されます。理想的には一様分布になります。\n",
    "  <br>\n",
    "  <img src=\"imgs/histogram.png\" width=\"300\">\n",
    "  <br>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のコードブロックでは、レイヤーの定義、重みの初期化、学習、記録の処理を全て記述しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VQ_AutoEncoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,h):\n",
    "        super().__init__()\n",
    "        self.model_name = h.model_name\n",
    "        self.h = h\n",
    "        self.num_quantizing = h.num_quantizing\n",
    "        self.quantizing_dim = h.quantizing_dim\n",
    "        self.lr = h.lr\n",
    "        self.my_hparams_dict = h.get()\n",
    "\n",
    "        # set criterion\n",
    "        self.reconstruction_loss = nn.MSELoss()\n",
    "        self.quantizing_loss = nn.MSELoss()\n",
    "        \n",
    "        # set histogram\n",
    "        self.q_hist = torch.zeros(self.num_quantizing,dtype=torch.int)\n",
    "        self.q_hist_idx = np.arange(self.num_quantizing)\n",
    "        # set layers\n",
    "        self.encoder = Encoder(h)\n",
    "        self.quantizer = Quantizing(h.num_quantizing,h.quantizing_dim)\n",
    "        self.decoder = Decoder(h)\n",
    "\n",
    "        self.input_size = self.encoder.input_size\n",
    "        self.output_size = self.input_size\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        h = self.encoder(x)\n",
    "        Qout,Qidx = self.quantizer(h)\n",
    "        y = self.decoder(Qout)\n",
    "        return y\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(),self.lr)\n",
    "        return optim\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def set_quantizing_weight(self,data_loader,device='cpu'):\n",
    "        self.quantizer.is_initialized = False\n",
    "        for batch in data_loader:\n",
    "            data,_ = batch\n",
    "            data = data.to(device)\n",
    "            Eout = self.encoder(data)\n",
    "            self.quantizer.initialize_weight(Eout)\n",
    "            if self.quantizer.is_initialized:\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def on_fit_start(self) -> None:\n",
    "        self.logger.log_hyperparams(self.my_hparams_dict)\n",
    "\n",
    "    def training_step(self,batch,idx):\n",
    "        data,_  = batch\n",
    "        self.view_data = data\n",
    "        Eout = self.encoder(data)\n",
    "        Qtgt = Eout.detach()\n",
    "        Qout,Qidx = self.quantizer(Qtgt)\n",
    "        out = self.decoder(Eout)\n",
    "\n",
    "        # loss\n",
    "        r_loss = self.reconstruction_loss(out,data)\n",
    "        q_loss = self.quantizing_loss(Qout,Qtgt)\n",
    "        loss = r_loss + q_loss\n",
    "\n",
    "        # log\n",
    "        rq_loss = self.reconstruction_loss(self.decoder(Qout),data)\n",
    "        self.log('loss',loss)\n",
    "        self.log('reconstruction_loss',r_loss)\n",
    "        self.log('quantizing_loss',q_loss)\n",
    "        self.log('reconstructed_quantizing_loss',rq_loss)\n",
    "\n",
    "        idx,count = torch.unique(Qidx,return_counts = True)\n",
    "        self.q_hist[idx.cpu()] += count.cpu()\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def on_epoch_end(self) -> None:\n",
    "        if (self.current_epoch+1) % self.h.view_interval ==0:\n",
    "            # image logging\n",
    "            data = self.view_data[:self.h.max_view_imgs].float()\n",
    "            data_len = len(data)\n",
    "            Eout = self.encoder(data)\n",
    "            Qout,Qidx = self.quantizer(Eout)\n",
    "            out = self.decoder(Eout)\n",
    "            Qdecoded = self.decoder(Qout)\n",
    "\n",
    "            grid_img = make_grid(torch.cat([data,out,Qdecoded],dim=0),nrow=data_len)\n",
    "            self.logger.experiment.add_image(\"MNIST Quantizings\",grid_img,self.current_epoch)\n",
    "\n",
    "            # histogram logging\n",
    "            fig = plt.figure(figsize=(6.4,4.8))\n",
    "            ax = fig.subplots()\n",
    "            ax.bar(self.q_hist_idx,self.q_hist)\n",
    "            \n",
    "            quantized_num = len(self.q_hist[self.q_hist!=0])\n",
    "            q_text = f'{quantized_num}/{self.num_quantizing}'\n",
    "            ax.text(0.9,1.05,q_text,ha='center',va='center',transform=ax.transAxes,fontsize=12)\n",
    "            ax.set_xlabel('weight index')\n",
    "            ax.set_ylabel('num')\n",
    "            self.logger.experiment.add_figure('Quantizing Histogram',fig,self.current_epoch)\n",
    "            \n",
    "        self.q_hist.zero_()\n",
    "\n",
    "    def summary(self,tensorboard=False):\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        dummy = torch.randn(self.input_size)\n",
    "        summary(self,dummy)\n",
    "\n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(comment=self.model_name,log_dir=\"VQAE_log\")\n",
    "            writer.add_graph(self,dummy)\n",
    "            writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の処理を簡単に書くために、`pytorch-lightning`というライブラリを使用しています。  \n",
    "`pytorch-lightning`では下のような形でモデルを定義することができます。  \n",
    "```python\n",
    "class ModelName(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, arguments): # required\n",
    "        super().__init__()\n",
    "        ################################################\n",
    "        # この中でレイヤーや、criterion(Loss関数)を定義します。\n",
    "        self.criterion = SomeLossFunc()\n",
    "        self.layer = SomeLayers()\n",
    "        ################################################\n",
    "\n",
    "    def forward(self, input): # not required (when training)\n",
    "        ################################################\n",
    "        # この中にデータの流れを書きますが、学習する際にこの\n",
    "        # forward methodは使われません。\n",
    "        output = self.layer(input)\n",
    "        ################################################\n",
    "        return output\n",
    "\n",
    "    def configure_optimizers(self): # required\n",
    "        ################################################\n",
    "        # この中でoptimizerを定義して、returnで返します。\n",
    "        optim = torch.optim.SomeOptimizer(self.parameters(), lr=lr)\n",
    "        ################################################\n",
    "        return optim\n",
    "    \n",
    "    def training_step(self, batch, idx): # required\n",
    "        ################################################\n",
    "        # この中で学習する時のデータの流れを書きます。学習するとき\n",
    "        # は\"損失\"まで計算し、それを return で返します。学習時に\n",
    "        # 記録したい値は self.log(\"name\", value) で記録するこ\n",
    "        # とができます。\n",
    "        input, answer = batch # extracting\n",
    "        output = self.layer(input) # flow\n",
    "        loss = self.criterion(output, answer) # calculate loss\n",
    "        self.log(\"loss\",loss) # log\n",
    "        return loss # return (required)\n",
    "        ################################################\n",
    "\n",
    "    def validation_step(self, batch, idx): # not required when you don't need the validation.\n",
    "        ################################################\n",
    "        # この中に検証用データセットでの処理を書きます。この関数で\n",
    "        # は値をreturnしなくて良いです。self.log(\"name\", value)\n",
    "        # で値を記録してください。\n",
    "        # Trainerにvalidation用のDataLoaderを与えなかった場合、\n",
    "        # この関数はつかわれません。　\n",
    "        ################################################\n",
    "```\n",
    "\n",
    "Event drivenなので、他にも様々な関数が用意されています。pytorch_lightningの`Trainer`が自動的にそのタイミングでオーバーライドされた関数を呼び出します。\n",
    "```python\n",
    "model = ModelName(arguments)\n",
    "trainer = pl.Trainer(gpus=1,precsion=16, max_epochs=10,...)\n",
    "trainer.fit(model,train_loader, validation_loader)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper parameter の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class hparam:\n",
    "    model_name:str = \"VQ_AutoEncoder\"\n",
    "    max_view_imgs = 16\n",
    "    view_interval = 10\n",
    "\n",
    "    lr:float = 0.001\n",
    "\n",
    "    num_quantizing:int = 32\n",
    "    quantizing_dim:int = 32\n",
    "\n",
    "    channels:int = 1\n",
    "    width:int = 28\n",
    "    height:int = 28\n",
    "\n",
    "    def get(self):\n",
    "        return self.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tensor Board\n",
    " Google Colabでこのノートブックを実行している方は、次のコードブロックを実行してください。  \n",
    " ローカルマシンで実行している方はノートブックと同じディレクトリでターミナルを開き、次のコマンドを実行してください。    \n",
    " `tensorboard --logdir=\"VQAE_log\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"VQAE_log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットのロード(MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "dataset = MNIST(\n",
    "    \"data\",train=False,download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 時刻を取得する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_now(strf:str = '%Y-%m-%d_%H-%M-%S'):\n",
    "    now = datetime.now().strftime(strf)\n",
    "    return now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パラメータの保存について"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_dir():\n",
    "    if not os.path.exists(\"params\"):\n",
    "        os.makedirs(\"params\")\n",
    "        \n",
    "def save_params(model:VQ_AutoEncoder, now):\n",
    "    param_dir()\n",
    "    torch.save(model.encoder.state_dict(),f\"params/{model.model_name}_{now}.encoder.pth\")\n",
    "    torch.save(model.decoder.state_dict(),f\"params/{model.model_name}_{now}.decoder.pth\")\n",
    "    torch.save(model.quantizer.state_dict(),f\"params/{model.model_name}_{now}.quantizing.pth\")\n",
    "    torch.save(model.state_dict(),f\"params/{model.model_name}_{now}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データローダーの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "dataloader = dutil.DataLoader(\n",
    "    dataset, BATCH_SIZE,shuffle=True, num_workers=0, pin_memory=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch lightning の key words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_kwds = {\n",
    "    \"gpus\": 1 if torch.cuda.is_available() else 0,\n",
    "    \"precision\": 16,\n",
    "    \"max_epochs\": 500,\n",
    "    \"log_every_n_steps\":5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習（1回目）\n",
    "一回目の学習をしていきましょう。ネタバレですが、うまく量子化できません。ですが学習後のEncoderとDecoderの重みを使用するため必ず実行してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### インスタンス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hparam(model_name=\"VQAE_pure\")\n",
    "model = VQ_AutoEncoder(h)\n",
    "model.summary(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantinzing Weight をセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_quantizing_weight(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl_loggers.TensorBoardLogger(\"VQAE_log/pure\")\n",
    "trainer = pl.Trainer(logger=logger, **pl_kwds)\n",
    "trainer.fit(model, dataloader)\n",
    "logger.close()\n",
    "now = get_now()\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### パラメータ保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(model,now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### なぜうまくいかないのか\n",
    "このようなヒストグラムは確認できましたか？  \n",
    "結果をTensorboardで確認するとわかりますが、全くクラスタリングできていません。全てのデータがただ一つの量子化ベクトルに割当たってしまっています。  \n",
    "この原因は学習初期のEncoderの出力分布が大きく変化するためでしょう。量子化ベクトルがEncoderの出力$E_{out}$の変化を追いきれないのです。\n",
    "<br>\n",
    "<img src=\"imgs/failed.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習（通常）\n",
    "ということで、ほとんど学習が終わったEncoderとDecoderの重みを使いましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### パラメータのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_param(model:VQ_AutoEncoder):\n",
    "    model.encoder.load_state_dict(torch.load(\"params/~.encoder.pth\"))    \n",
    "    model.decoder.load_state_dict(torch.load(\"params/~.decoder.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### インスタンスと実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(settings:Dict[str,Any] = dict(), pl_kwds:Dict[str, Any]=dict()):\n",
    "    h = hparam(**settings)\n",
    "    model = VQ_AutoEncoder(h)\n",
    "    load_trained_param(model)\n",
    "    model.set_quantizing_weight(dataloader)\n",
    "    logger = pl_loggers.TensorBoardLogger(\"VQAE_log/fromTraining\")\n",
    "    trainer = pl.Trainer(logger=logger, **pl_kwds)\n",
    "    trainer.fit(model, dataloader)\n",
    "    now = get_now()\n",
    "    logger.close()\n",
    "    save_params(model, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_kwds = {\n",
    "    \"gpus\": 1 if torch.cuda.is_available() else 0,\n",
    "    \"precision\": 16,\n",
    "    \"max_epochs\": 100,\n",
    "    \"log_every_n_steps\":5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(pl_kwds=pl_kwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ex) changing `num_quantizing`\n",
    "量子化数を変化させて結果がどのように変わるか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_quantizings = [8,128,512]\n",
    "for nq in num_quantizings:\n",
    "    setting = {\n",
    "        \"model_name\":\"vqae_changing_num_quantizing_{}\".format(nq),\n",
    "        \"num_quantizing\":nq,\n",
    "    }\n",
    "    train(setting, pl_kwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果を解析する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考えること"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4467f1926f03d6fcd44316305074fb8c0b3cdc10977f45ea461091401ebc85a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('JARVIS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
